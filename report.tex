\documentclass[a4paper,11pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}

\title{Building data.eurecom.fr}
\author{Anne-Elisabeth Gazet}
\date{Fall 2011}

\begin{document}
\maketitle

\chapter*{Introduction}

\section*{Context}
The web as we know it today mainly consists of documents that are linked together. We are now moving to a new era, where raw data is being published on the web as \emph{Linked Data}, and woven into the \emph{Web of Data} or \emph{Semantic Web}. 
\begin{quotation}
``The vision of the Semantic Web is to extend principles of the Web from documents to data. Data should be accessed using the general Web architecture using, e.g., URI-s; data should be related to one another just as documents (or portions of documents) are already. This also means creation of a common framework that allows data to be shared and reused across application, enterprise, and community boundaries, to be processed automatically by tools as well as manually, including revealing possible new relationships among pieces of data.''
\begin{flushright}
from the W3C Semantic Web FAQ\footnote{http://www.w3.org/2001/sw/SW-FAQ}
\end{flushright}
\end{quotation}
In order to achieve these goals, new technologies were developped:
\begin{itemize}
\item RDF, for Resource Description Framework, is the main building block of the Semantic Web. It's a simple interchange format, where data is represented as triples of the form (subject, predicate, object) ;
\item OWL (Web Ontology Language) and RDFS (RDF Schema) are vocabularies for describing RDF properties and classes ;
\item SPARQL (SPARQL Protocol and RDF Query Language) is a protocol and query language for semantic web data sources ;
\item SPARQL Update, a companion language for SPARQL, enables modifying a data source. %(It should soon become a W3C recommendation.)
\end{itemize}
%
%Tom Heath and Christian Bizer (2011) Linked Data: Evolving the Web into a Global Data Space (1st edition). Synthesis Lectures on the Semantic Web: Theory and Technology, 1:1, 1-136. Morgan & Claypool, http://linkeddatabook.com/editions/1.0/

\section*{Motivation for the project}
There is a trend to increase government transparency by releasing more and more public sector information on the web as linked data. The first initiative of the kind was data.gov.uk in the United Kingdom, which was followed by similar projects around the world. 

Universities and schools also generate a lot of data about students, promotions, professors, courses, publications, departments, rooms, schedules, exams, etc. The goal of this project is to take all this data generated by EURECOM, transform it in semantic formats (RDF), interlink it with other data (from other universities and datasets) and publish the whole as linked data in order to develop showcase applications that provide useful services to students and professors. %Ultimately, a mobile application could be developed.

\section*{Contents}

\chapter{Similar projects}
%Shorten this chapter, and just put short description of the projects, and the conclusions as found in the presentation from week 1 ?

This project was inspired by recent similar initiatives in other universities. I studied them in order to see what kind of data they made available, which approach they took in order to publish their data, and what kind of applications were built thanks to the newly exposed data. 
\section*{data.open.ac.uk - open linked data from The Open University}
The OU's LUCERO (Linking University Content for Education and Research Online) project\footnote{http://lucero-project.info/lb/} was the first initiative to expose public information from a university as Linked Open Data. The data.open.ac.uk platform was developped as part of the LUCERO project. 

Datasets exposed on this platform include:
\begin{description}
\item{Open Research Online:} publications from OU researchers ;
\item{OU podcasts:} collection of Audio and Video material related to education and research at the Open University ;
\item{Course Descriptions ;}
\item{OpenLearn:} metadata related to units of teaching and learning material openly available from the OpenLearn website ;
\item{KMi Planet Stories:} from the online news system of the Knowledge Media Institute ;
\item{KMi People Profiles.}
\end{description}

Vocabularies used to represent the data include:
\begin{description}
\item{BibO:} the Bibliographic Ontology\footnote{http://bibliontology.com/specification} is used to represent information about publications originating from OU researchers ;
\item{W3C Media Ontology:} this ontology\footnote{http://www.w3.org/TR/mediaont-10/} is used to describe audio and video material in the OU podcasts dataset ;
\item{AIISO and the courseware ontology:} the Academic Institution Internal Structure Ontology\footnote{http://vocab.org/aiiso/} and the courseware ontology\footnote{http://courseware.rkbexplorer.com/ontologies/courseware} are used to describe courses ;
\item{FOAF:} the Friend Of A Friend ontology\footnote{http://xmlns.com/foaf/spec/} is used to represent information on the staff members at the Knowledge Media Institute
\end{description}

Applications
\begin{itemize}
\item OpenLearn Linked Data\footnote{http://fouad.zablith.org/apps/openlearnlinkeddata/} makes use of data from data.open.ac.uk to suggest courses, podcasts and other OpenLearn units that relate to an OpenLearn Unit ;
\item The OU Expert Search\footnote{http://kmi-web15.open.ac.uk:8080/ExpertSearchClient/ (accessible inside the OU network only)} system allows users to find academics at the Open University who are experts in a given domain ;
\item OUExperts\footnote{http://vimeo.com/19743762} is a mobile (android) application to find Open Univeristy experts in a given domain, and connect to their social network ;
\item Buddy Study\footnote{http://www.matthew-rowe.com/BuddyStudy/} suggests potential contacts and Open University courses to follow for students, based on the analysis of the topics in the user's Facebook page.
\end{itemize}

\section*{data.southampton.ac.uk}
\section*{data.ox.ac.uk}
\section*{LODUM - Linked Open Data University of MÃ¼nster}
\section*{National Research Council (CNR) - data.cnr.it}
\section*{Conclusion}



\chapter{Data model and design of our ontology}
\section{Available data}
One of the difficulties with this project is that we had to think about the data model before actually getting access to the data. To have a clearer idea of what data was stored at EURECOM, I had a look at the internet and intranet sites of the institute. The information available there is mainly about people (teachers, researchers, doctoral students, staff members), research outputs and courses. Assuming these would be the datasets we could access later on, we designed an RDF data model. The aim of this model is to fit the data accurately, as well as to make the data reusable by third parties. 
\section{Choice of external vocabularies}
\section{REVE - the Research and Education Vocabulary for EURECOM}
\section{First data model}
%graphs, graphs, graphs
\section{Present state of the data}
%graph of what can actually be found in the triplestore at the moment



\chapter{Data conversion}
%Quick word on R2RDF somewhere
\section{Accessing the data} 
%Quickly describe the web service, show example of JSON
\section{Building RDF triples with Python}
%Justify choice of Python : easy and fast to test things, good existing tools to work with RDF, and for now we don't care much about fast-executing code since the conversion is done once and for all - it's more about testing feasibility
%A few words on rdflib
%Snippets of code, explain conversion strategy
\section{Enriching the data with an external source: GeoNames}
%Explain choice of source : widely used service, URIs for the resources that can be built using their geocoding webservice, thus fitting the data we had at hand (country code and city name)
%Show code and GeoNames's WS URL
%They don't have their own official SPARQL endpoint so we store their (lat, long) data locally. (Would it be worth it to store everything ?)
\section{Loading triples in a triplestore using SPARQL Update}



\chapter{Showcase application : a map of EURECOM publications}
%Intro: there are no killer app for the SW, it enables to build many new services, it's up to the programmer to get inspiration from the data
%Present Exhibit
%Explain how the data is queried (show queries) and translated as JSON for Exhibit
%Screenshots & url
%Needed improvements



\chapter{Future work}
%Improve on architecture
%Expose more data
%Clean up data
%Align with other datasets
%Build more applications

\end{document}
