\documentclass[a4paper,11pt]{report}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\bibliographystyle{unsrt}

\title{Building data.eurecom.fr}
\author{Anne-Elisabeth Gazet}
\date{Fall 2011}

\begin{document}
\maketitle

\chapter*{Introduction}

\section*{Context}
The web as we know it today mainly consists of documents that are linked together. We are now moving to a new era, where raw data is being published on the web as \emph{Linked Data}, and woven into the \emph{Web of Data} or \emph{Semantic Web}. 
\begin{quotation}
``The vision of the Semantic Web is to extend principles of the Web from documents to data. Data should be accessed using the general Web architecture using, e.g., URI-s; data should be related to one another just as documents (or portions of documents) are already. This also means creation of a common framework that allows data to be shared and reused across application, enterprise, and community boundaries, to be processed automatically by tools as well as manually, including revealing possible new relationships among pieces of data.''
\begin{flushright}
from the W3C Semantic Web FAQ\footnote{http://www.w3.org/2001/sw/SW-FAQ}
\end{flushright}
\end{quotation}
In order to achieve these goals, new technologies were developed:
\begin{itemize}
\item RDF, for Resource Description Framework, is the main building block of the Semantic Web. It's a simple interchange format, where data is represented as triples of the form (subject, predicate, object) ;
\item OWL (Web Ontology Language) and RDFS (RDF Schema) are vocabularies for describing RDF properties and classes ;
\item SPARQL (SPARQL Protocol and RDF Query Language) is a protocol and query language for semantic web data sources ;
\item SPARQL Update, a companion language for SPARQL, enables modifying a data source. %(It should soon become a W3C recommendation.)
\end{itemize}

See~\cite{Heath2011} if you want to learn more about the Linked Data paradigm. 

\section*{Motivation for the project}
There is a trend to increase government transparency by releasing more and more public sector information on the web as linked data. The first initiative of the kind was data.gov.uk in the United Kingdom, which was followed by similar projects around the world. 

Universities and schools also generate a lot of data about students, promotions, professors, courses, publications, departments, rooms, schedules, exams, etc. The goal of this project is to take all this data generated by EURECOM, transform it in semantic formats (RDF), interlink it with other data (from other universities and datasets) and publish the whole as linked data in order to develop showcase applications that provide useful services to students and professors. %Ultimately, a mobile application could be developed.

\section*{Contents}

\chapter{Similar projects}
%Shorten this chapter, and just put short description of the projects, and the conclusions as found in the presentation from week 1 ?

This project was inspired by recent similar initiatives in other universities. I studied them in order to see what kind of data they made available, which approach they took in order to publish their data, and what kind of applications were built thanks to the newly exposed data. 
\section*{data.open.ac.uk - open linked data from The Open University}
The OU's LUCERO (Linking University Content for Education and Research Online) project\footnote{http://lucero-project.info/lb/} was the first initiative to expose public information from a university as Linked Open Data. The data.open.ac.uk platform was developed as part of the LUCERO project. 

Datasets exposed on this platform include:
\begin{description}
\item{Open Research Online:} publications from OU researchers ;
\item{OU podcasts:} collection of Audio and Video material related to education and research at the Open University ;
\item{Course Descriptions ;}
\item{OpenLearn:} metadata related to units of teaching and learning material openly available from the OpenLearn website ;
\item{KMi Planet Stories:} from the online news system of the Knowledge Media Institute ;
\item{KMi People Profiles.}
\end{description}

Vocabularies used to represent the data include:
\begin{description}
\item{BibO:} the Bibliographic Ontology\footnote{http://bibliontology.com/specification} is used to represent information about publications originating from OU researchers ;
\item{W3C Media Ontology:} this ontology\footnote{http://www.w3.org/TR/mediaont-10/} is used to describe audio and video material in the OU podcasts dataset ;
\item{AIISO and the courseware ontology:} the Academic Institution Internal Structure Ontology\footnote{http://vocab.org/aiiso/} and the courseware ontology\footnote{http://courseware.rkbexplorer.com/ontologies/courseware} are used to describe courses ;
\item{FOAF:} the Friend Of A Friend ontology\footnote{http://xmlns.com/foaf/spec/} is used to represent information on the staff members at the Knowledge Media Institute
\end{description}

Applications
\begin{itemize}
\item OpenLearn Linked Data\footnote{http://fouad.zablith.org/apps/openlearnlinkeddata/} makes use of data from data.open.ac.uk to suggest courses, podcasts and other OpenLearn units that relate to an OpenLearn Unit ;
\item The OU Expert Search\footnote{http://kmi-web15.open.ac.uk:8080/ExpertSearchClient/ (accessible inside the OU network only)} system allows users to find academics at the Open University who are experts in a given domain ;
\item OUExperts\footnote{http://vimeo.com/19743762} is a mobile (android) application to find Open Univeristy experts in a given domain, and connect to their social network ;
\item Buddy Study\footnote{http://www.matthew-rowe.com/BuddyStudy/} suggests potential contacts and Open University courses to follow for students, based on the analysis of the topics in the user's Facebook page.
\end{itemize}

\section*{data.southampton.ac.uk}
\section*{data.ox.ac.uk}
\section*{LODUM - Linked Open Data University of MÃ¼nster}
\section*{National Research Council (CNR) - data.cnr.it}
\section*{Conclusion}



\chapter{Data model and design of our ontology}
\section{Available data}
One of the difficulties with this project is that we had to think about the data model before actually getting access to the data. To have a clearer idea of what data was stored at EURECOM, I had a look at the internet and intranet sites of the institute. The information available there is mainly about: 
\begin{itemize}
\item people (teachers, researchers, doctoral students, staff members), 
\item research outputs, 
\item courses. 
\end{itemize}
Assuming these would be the datasets we would access later on, we designed an RDF data model. The aim of this model is to fit the data accurately, as well as to make the data reusable by third parties. 
 
\section{Choice of external vocabularies}
To achieve re-usability, it is paramount to represent data using vocabularies that are used by other projects as well. This way, the data will be easier to consume for existing applications. Furthermore, the fact that a vocabulary was accepted by the community as a de facto standard is a good hint that this vocabulary is well designed, and well fitted to its domain. 

%tools to find out about vocabs : 

Yet, since there are still relatively few universities publishing their data as Linked Data, we could not rely only on popularity to make choices. Besides, each dataset being unique, there are relationships that we wanted to represent, to which we did not find any equivalent in the other projects' datasets. In this case, we used tools such as Schema-Cache\footnote{http://schemacache.com/}, Schemapedia\footnote{http://schemapedia.com/} and Sindice\footnote{http://sindice.com/} to see if there existed any term we could use. Alongside popularity, the other criteria we took into account to select vocabularies are:
\begin{itemize}
\item the similitude between the concepts and terms present in the vocabulary, and the concepts needed to represent the data ;
\item whether the vocabulary was created as part of a bigger project: this means it is the result of a consensus rather than the vision of a sole individual on the domain.
\end{itemize}

In the end, we selected the following vocabularies: 
\begin{itemize}
\item FOAF\footnote{http://xmlns.com/foaf/spec/}: the Friend Of A Friend vocabulary defines terms for describing persons and their relations to other people and objects. We use it to describe people at EURECOM. 
\item DC terms\footnote{http://dublincore.org/documents/dcmi-terms/}
\item Participation\footnote{http://vocab.org/participation/schema}: The participation ontology is a simple model for describing the roles that people play within groups. As discussed later, we subclass the class part:Role in the REVE ontology, to describe roles played by people inside EURECOM. 
\item AIISO\footnote{http://vocab.org/aiiso/schema}: The Academic Institution Internal Structure Ontology provides classes and properties to describe the internal organizational structure of an academic institution. We use the aiiso:Course and aiiso:KnowledgeGrouping classes to define courses and tracks. 
\item BIBO\footnote{http://bibliontology.com/specification}:  The Bibliographic Ontology describe bibliographic things on the semantic Web in RDF. It has been inspired by many existing document description metadata formats. We use it to describe the articles in EURECOM's scientific publications repository. 
\item LODE\footnote{http://linkedevents.org/ontology/}: The ontology for Linking Open Descriptions of Events is an ontology for publishing descriptions of historical events as Linked Data, and is designed to be compatible with other event-related vocabularies and ontologies. We use the predicates lode:atTime, lode:atPlace and lode:involvedAgent to describe course sessions. 
%check if it's involved or involvedAgent
%\item DUL
\item OWL-Time\footnote{http://www.w3.org/TR/owl-time/}
\item Rooms\footnote{http://vocab.deri.ie/rooms}
\end{itemize}
\section{REVE - the Research and Education Vocabulary for EURECOM}
\section{First data model}
%graphs, graphs, graphs
\section{Present state of the data}
%graph of what can actually be found in the triplestore at the moment



\chapter{Data conversion}
%A word on naming the entities somewhere
%Quick word on R2RDF somewhere
\section{Accessing the data} 
%Quickly describe the web service, show example of JSON
\section{Building RDF triples with Python}
%Justify choice of Python : easy and fast to test things, good existing tools to work with RDF, and for now we don't care much about fast-executing code since the conversion is done once and for all - it's more about testing feasibility
%A few words on rdflib
%Snippets of code, explain conversion strategy
\section{Enriching the data with an external source: GeoNames}
%Explain choice of source : widely used service, URIs for the resources that can be built using their geocoding webservice, thus fitting the data we had at hand (country code and city name)
%Show code and GeoNames's WS URL
%They don't have their own official SPARQL endpoint so we store their (lat, long) data locally. (Would it be worth it to store everything ?)
\section{Loading triples in a triplestore using SPARQL Update}



\chapter{Showcase application : a map of EURECOM publications}
%Intro: there are no killer app for the SW, it enables to build many new services, it's up to the programmer to get inspiration from the data
%Present Exhibit
%Explain how the data is queried (show queries) and translated as JSON for Exhibit
%Screenshots & url
%Needed improvements



\chapter{Future work}
%Improve on architecture
%Expose more data
%Clean up data
%Align with other datasets
%Build more applications
\bibliography{biblio}
\end{document}
